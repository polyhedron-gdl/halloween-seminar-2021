{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d06289b",
   "metadata": {},
   "source": [
    "# Classification for Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e631666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b081b6",
   "metadata": {},
   "source": [
    "Classification is a primary form of text analysis and is widely used in a variety of domains and applications. The premise of classification is simple: given a categorical target variable, learn patterns that exist between instances composed of independent variables and their relationship to the target. Because the target is given ahead of time, classification is said to be supervised machine learning because a model can be trained to minimize error between predicted and actual categories in the training data. Once a classification model is fit, it assigns categorical labels to new instances based on the patterns detected during training.\n",
    "\n",
    "This simple premise gives the opportunity for a huge number of possible applications, so long as the application problem can be formulated to identify either a yes/no (binary classification) or discrete buckets (multiclass classification). The most difficult part of applied text analytics is the curation and collection of a domain-specific corpus to build models upon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b4c2f",
   "metadata": {},
   "source": [
    "All classifier model families have the same basic workflow, and with Scikit-Learn Estimator objects, they can be employed in a procedural fashion and compared using cross-validation to select the best performing predictor.\n",
    "\n",
    "The classification workflow occurs in two phases: a build phase and an operational phase. \n",
    "\n",
    "- In the build phase, a corpus of documents is transformed into feature vectors. The document features, along with their annotated labels (the category or class we want the model to learn), are then passed into a classification algorithm that defines its internal state along with the learned patterns. \n",
    "\n",
    "- Once trained or fitted, a new document can be vectorized into the same space as the training data and passed to the predictive algorithm, which returns the assigned class label for the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3cb851",
   "metadata": {},
   "source": [
    "![image.png](./img/3_2_text_classification_pic_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab6e0e",
   "metadata": {},
   "source": [
    "This is a very practical topic so let's start studying some concrete examples right away. In the rest of this section, we will look at how classifiers can be employed to solve a wide variety of tasks. Our discussion is not intended to be comprehensive, but to give a representative sample of tasks that can be performed with the help of text classifiers.\n",
    "\n",
    "In any case remember that choosing an appropriate classification algorithm for a particular problem task requires\n",
    "practice and experience; each algorithm has its own quirks and is based on certain\n",
    "assumptions. To restate the **no free lunch theorem** by David H. Wolpert, ***no single\n",
    "classifier works best across all possible scenarios*** *(The Lack of A Priori Distinctions\n",
    "Between Learning Algorithms, Wolpert, David H, Neural Computation 8.7 (1996): 1341-\n",
    "1390)*. In practice, it is always recommended that you **compare the performance\n",
    "of at least a handful of different learning algorithms to select the best model for\n",
    "the particular problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63883706",
   "metadata": {},
   "source": [
    "## Example 1 - Gender Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ec36f",
   "metadata": {},
   "source": [
    "**Reference**: *Bird S. et al. \"Natural Language Processing with Python\" O'Reilly (2009), Chapter 6 - \"Learning to Classify Text\"* and references therein. [Here](https://www.nltk.org/book/ch06.html) the on-line version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a738db9",
   "metadata": {},
   "source": [
    "**Focus**\n",
    "\n",
    "- what is a feature in text analysis\n",
    "- training Vs validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f99a7",
   "metadata": {},
   "source": [
    "In this first example we will try to build a simple algorithm to understand if a noun passed in input is of masculine or feminine gender. In this, and in the following examples, we will implicitly assume that the language used is **English**.\n",
    "\n",
    "Usually one of the first step in creating a classifier is deciding what features of the input are relevant, and how to encode those features. For this example, we'll start by just **looking at the final letter of a given name**. The following **feature extractor** function builds a dictionary containing relevant information about a given name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b046f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features_1(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab48cb3",
   "metadata": {},
   "source": [
    "The returned **dictionary**, known as a **feature set**, maps from feature names to their values. Feature names are case-sensitive strings that typically provide a short human-readable description of the feature, as in the example **'last_letter'**. Feature values are values with simple types, such as booleans, numbers, and strings, in this case the last letter of the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad10ac21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'n'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features_1(\"batman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c428a",
   "metadata": {},
   "source": [
    "Now that we've defined a feature extractor, we need to prepare a list of examples and corresponding class labels. For this we are going to use the corpus 'names' which is part of nltk corpus collection. In particular the names corpus contains a total of around 2943 male (male.txt) and 5001 female (female.txt) names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0a364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of male names:\n",
      "2943\n",
      "\n",
      "Number of female names:\n",
      "5001\n",
      "\n",
      "First 10 male names:\n",
      "['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim', 'Abdullah', 'Abe', 'Abel', 'Abelard', 'Abner']\n",
      "\n",
      "First 10 female names:\n",
      "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale', 'Abra', 'Acacia', 'Ada', 'Adah', 'Adaline']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names \n",
    "\n",
    "print(\"\\nNumber of male names:\")\n",
    "print (len(names.words('male.txt')))\n",
    "print(\"\\nNumber of female names:\")\n",
    "print (len(names.words('female.txt')))\n",
    "\n",
    "male_names   = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "\n",
    "print(\"\\nFirst 10 male names:\")\n",
    "print (male_names[0:15])\n",
    "print(\"\\nFirst 10 female names:\")\n",
    "print (female_names[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bd5fe",
   "metadata": {},
   "source": [
    "Associate to each name the corresponding label ('male' or 'female') and shuffle the set disrupting alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7d1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in male_names] + \n",
    "                 [(name, 'female') for name in female_names])\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a5d918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Oren', 'male'),\n",
       " ('Enrichetta', 'female'),\n",
       " ('Maryl', 'female'),\n",
       " ('Warde', 'male'),\n",
       " ('Aime', 'female'),\n",
       " ('Colene', 'female'),\n",
       " ('Barnabe', 'male'),\n",
       " ('Griffin', 'male'),\n",
       " ('Aldrich', 'male'),\n",
       " ('Britani', 'female'),\n",
       " ('Hendrik', 'male'),\n",
       " ('Keriann', 'female'),\n",
       " ('Shelley', 'male'),\n",
       " ('Joline', 'female'),\n",
       " ('Matthus', 'male')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_names[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294242e",
   "metadata": {},
   "source": [
    "Next, we use the feature extractor to process the names data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e2bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features_1(n), gender) for (n, gender) in labeled_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec234d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'male'),\n",
       " ({'last_letter': 'n'}, 'male'),\n",
       " ({'last_letter': 'h'}, 'male'),\n",
       " ({'last_letter': 'i'}, 'female')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e08ca",
   "metadata": {},
   "source": [
    "and divide the resulting list of feature sets into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a2abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb7d3d",
   "metadata": {},
   "source": [
    "The training set is used to train a \"naive Bayes\" classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171dd024",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a7c7bad752ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83801dad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8f7694f6cc39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgender_features_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Frodo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.classify(gender_features_1('Frodo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346b511c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8c37647c7d73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgender_features_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Trinity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.classify(gender_features_1('Trinity'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89a221",
   "metadata": {},
   "source": [
    "We can systematically evaluate the classifier on a much larger quantity of unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb642588",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ee1353d56882>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18580cd",
   "metadata": {},
   "source": [
    "Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method's ability to extract a good model. Much of the interesting work in building a classifier is deciding what features might be relevant, and how we can represent them. Although it's often possible to get decent performance by using a fairly simple and obvious set of features, there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand.\n",
    "\n",
    "Typically, feature extractors are built through a process of trial-and-error, guided by intuitions about what information is relevant to the problem. For example, it is clear that some suffixes that are more than one letter can be indicative of name genders. Names ending in 'yn' appear to be predominantly female (in english), despite the fact that names ending in 'n' tend to be male; and names ending in 'ch' are usually male, even though names that end in 'h' tend to be female. \n",
    "\n",
    "We therefore adjust our feature extractor to include features for two-letter suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0691fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features_2(word):     \n",
    "    return {'suffix1': word[-1:],\n",
    "            'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee979a",
   "metadata": {},
   "source": [
    "In this case we are going to use three set. First, we select a development set, containing the corpus data for creating the model. This development set is then subdivided into the training set and the dev-test set. The training set is used to train the model, and the dev-test set is used to perform error analysis. The test set serves in our final evaluation of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10499be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names   = labeled_names[1500:]\n",
    "devtest_names = labeled_names[500:1500]\n",
    "\n",
    "train_set     = [(gender_features_2(n), gender) for (n, gender) in train_names]\n",
    "devtest_set   = [(gender_features_2(n), gender) for (n, gender) in devtest_names]\n",
    "\n",
    "test_names    = labeled_names[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53706283",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e197ae68f23b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "classifier  = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130731d",
   "metadata": {},
   "source": [
    "And what about the 'test_names' dataset? Well, this analysis procedure can then be repeated, checking for patterns in the errors that are made by the newly improved classifier. Each time the error analysis procedure is repeated, we should select a different dev-test/training split, to ensure that the classifier does not start to reflect idiosyncrasies in the dev-test set.\n",
    "\n",
    "But once we've used the dev-test set to help us develop the model, we can no longer trust that it will give us an accurate idea of how well the model would perform on new data. It is therefore important to keep the test set separate, and unused, until our model development is complete. At that point, we can use the test set to evaluate how well our model will perform on new input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03375f2",
   "metadata": {},
   "source": [
    "## Example 2 - The «20 Newsgroup» dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57909b47",
   "metadata": {},
   "source": [
    "**References**: \n",
    "\n",
    "- *Javed Shaikh, \"Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK\"* [Towards Data Science](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a) and references therein.\n",
    "\n",
    "- [Scikit-Learn web site](https://scikit-learn.org/)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376a498",
   "metadata": {},
   "source": [
    "**Focus**\n",
    "\n",
    "- using sklearn package\n",
    "- numerical features and vectorization process\n",
    "- use of a Naive Bayes classifier\n",
    "- building a ml pipeline with sklearn\n",
    "- model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe507b",
   "metadata": {},
   "source": [
    "In this example we are going to work with the \"20 Newsgroup Dataset\". The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. [Here](http://qwone.com/~jason/20Newsgroups/) you can find the home page of the project and [here](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) the description of the scikit inclusion. This data set is infact in-built in scikit, so we don’t need to download it explicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb3144",
   "metadata": {},
   "source": [
    "### Loading the data set\n",
    "\n",
    "First of all load the training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dac8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a83d5a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names #prints all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629b83a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) #prints first line of the first data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1e7a5",
   "metadata": {},
   "source": [
    "### Extracting features from text files\n",
    "\n",
    "We already known that in order to run machine learning algorithms we need to convert the text files into numerical feature vectors. We will be using bag of words model for our example. Briefly, we segment each text file into words (for English splitting by space), and count # of times each word occurs in each document and finally assign each word an integer id. **Each unique word in our dictionary will correspond to a feature (descriptive feature)**.\n",
    "\n",
    "Remember that, scikit-learn has a high level component which will create feature vectors for us ‘CountVectorizer’. More about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Let's see a very simple example of 'CountVectorizer' before apply it to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee2b5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbc38a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2e983a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c172af3",
   "metadata": {},
   "source": [
    "Here by doing ‘vectorizer.fit_transform(corpus)’, we are learning the vocabulary dictionary and it returns a Document-Term matrix of dimension [n_samples, n_features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76822918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a671074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb4231",
   "metadata": {},
   "source": [
    "Now apply this vectorizer to our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69733728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276febf8",
   "metadata": {},
   "source": [
    "We can use also a measure based on Term Frequency matrix. In particular remember two important metrics of this type for Bag-of-Words model:\n",
    "\n",
    "- **TF**: Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency (TF - Term Frequencies) i.e. #count(word) / #Total words, in each document.\n",
    "\n",
    "- **TF-IDF**: Finally, we can even reduce the weightage of more common words like (the, is, an etc.) which occurs in all document. This is called as TF-IDF i.e Term Frequency times inverse document frequency.\n",
    "\n",
    "Remember that the goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "We can perform the calculation of TF-IDF matrix using below line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d6952a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4ba34ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bb9af",
   "metadata": {},
   "source": [
    "Below we use the 'TfidfTransformer' which transform a count matrix to a normalized tf or tf-idf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97d01e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabbf98",
   "metadata": {},
   "source": [
    "### Running ML algorithms.\n",
    "\n",
    "There are various algorithms which can be used for text classification. We will start with the most simplest one *Naive Bayes (NB)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36904269",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67f48d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "features    = X_train_tfidf\n",
    "labels      = twenty_train.target\n",
    "classifier  = MultinomialNB().fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f83ca",
   "metadata": {},
   "source": [
    "This will train the NB classifier on the training data we provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caaf2f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 130107)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "X_test_counts = count_vect.transform(twenty_test.data)         # transform metod NOT fit_transform\n",
    "X_test_tfidf  = tfidf_transformer.transform(X_test_counts)\n",
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c69c9f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important : </b>Note that for the test set we use the method 'transform' and not 'fit_transform' as before. In layman's terms the reason is that fit_transform means to do some calculation and then do transformation (say calculating the means of columns from some data and then replacing the missing values). So <b>for training set, you need to both calculate and do transformation</b>. But for <b>testing set</b>, the model applies prediction based on what was learned during the training set and so it doesn't need to calculate, it just performs the transformation. You can avoid many of these problems building a 'pipeline' as described above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e1c956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053b1f5",
   "metadata": {},
   "source": [
    "Now we will test the performance of the NB classifier on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecd7aaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caed85",
   "metadata": {},
   "source": [
    "#### Building a Pipeline\n",
    "\n",
    "The machine learning process often combines a series of transformers on raw data, transforming the dataset each step of the way until it is passed to the fit method of a final estimator. Pipeline objects enable us to integrate a series of transformers that combine normalization, vectorization, and feature analysis into a single, well-defined mechanism.\n",
    "\n",
    "The purpose of a Pipeline is to chain together multiple estimators representing a fixed sequence of steps into a single unit. All estimators in the pipeline, except the last one, must be transformers — that is, implement the transform method, while the last estimator can be of any type, including predictive estimators. Pipelines provide convenience; fit and transform can be called for single inputs across multiple objects at once. Pipelines also provide a single interface for grid search of multiple estimators at once. Most importantly, pipelines provide operationalization of text models by coupling a vectorization methodology with a predictive model. Pipelines are constructed by describing a list of (key, value) pairs where the key is a string that names the step and the value is the estimator object. Pipelines can be created either by using the make_pipeline helper function, which automatically determines the names of the steps, or by specifying them directly. Generally, it is better to specify the steps directly to provide good user documentation, whereas make_pipeline is used more often for automatic pipeline construction.\n",
    "\n",
    "To create a pipeline with sklearn we simply define the steps in the object I have called text_clf in the code below. We can then simply call fit on this object to train the model. The pipeline object can also be used to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2541ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa2a7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "656ab9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted   = text_clf.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50608bed",
   "metadata": {},
   "source": [
    "Removing stop words: (the, then etc) from the data. You should do this only when stop words are not useful for the underlying problem. In most of the text classification problems, this is indeed not useful. Let’s see if removing stop words increases the accuracy. Update the code for creating object of CountVectorizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86008f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8169144981412639"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted   = text_clf.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d270135",
   "metadata": {},
   "source": [
    "This is a case in which we have a great improvement removing stopwords from our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e2f28",
   "metadata": {},
   "source": [
    "### Model optimisation\n",
    "\n",
    "All estimators in the Scikit-learn library contain a range of parameters, for which there are multiple options. The values that you choose for a particular algorithm will impact how well the final model performs. For example, with the RandomForestClassifier you can set the max_depth of the tree to potentially any value, and depending on your data and task, different values for this parameter will produce different results.\n",
    "\n",
    "This process of trying different combinations of parameters to find the optimal combination is known as hyperparameter optimisation. Scikit-learn provides two tools to automatically perform this task, **GridSearchCV** which implements a technique known as exhaustive grid search and *RandomizedSearchCV* which performs randomized parameter optimisation.\n",
    "\n",
    "The below example uses GridSearchCV to find the optimal parameters of our simple Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "817914c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bde7f8",
   "metadata": {},
   "source": [
    "Here, we are creating a list of parameters for which we would like to do performance tuning. All the parameters name start with the classifier name (remember the arbitrary name we gave). E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal. Next, we create an instance of the grid search by passing the classifier, parameters and n_jobs=-1 which tells to use multiple cores from user machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71c6473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6b528",
   "metadata": {},
   "source": [
    "This might take few minutes to run depending on the machine configuration.\n",
    "Lastly, to see the best mean score and the params, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "144cef8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9129399133330441"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6059969c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078cfc3",
   "metadata": {},
   "source": [
    "**The accuracy has now increased to ~90.6%** for the NB classifier and the corresponding parameters are {‘clf__alpha’: 0.01, ‘tfidf__use_idf’: True, ‘vect__ngram_range’: (1, 2)}. alpha is the so-called Laplace Smoothing, for more infos see [here](https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09d7c7",
   "metadata": {},
   "source": [
    "## Example 4 - Amazon Review Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c3986",
   "metadata": {},
   "source": [
    "**Reference**: *Gunjit Bedi, \"A guide to Text Classification(NLP) using SVM and Naive Bayes with Python\", [Medium.com](https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34) and references therein.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c9f2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize                   import word_tokenize\n",
    "from nltk                            import pos_tag\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.corpus                     import wordnet as wn\n",
    "from nltk.stem                       import WordNetLemmatizer\n",
    "\n",
    "from collections                     import defaultdict\n",
    "\n",
    "from sklearn.preprocessing           import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn                         import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics                 import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75cf36d",
   "metadata": {},
   "source": [
    "We will use the an Amazon Review Data set which has 10,000 rows of Text data which is classified into “Label 1” and “Label 2”. The Data set has two columns “Text” and “Label”. You can download the data from [here](https://github.com/Gunjitbedi/Text-Classification/blob/master/corpus.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6501148",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-389ed32716bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./corpus/amazon_corpus.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "Corpus = pd.read_csv(\"./corpus/amazon_corpus.csv\",encoding='latin-1')\n",
    "Corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e0174",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "713afeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - a : Remove blank rows if any.\n",
    "Corpus['text'].dropna(inplace=True)\n",
    "\n",
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "Corpus['text'] = [entry.lower() for entry in Corpus['text']]\n",
    "\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "Corpus['text']= [word_tokenize(entry) for entry in Corpus['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "39c3465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8f519f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "# This a simple mapping from the output of pos_tag function and the tag format expected by the lemmatize method\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1817c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(Corpus['text']):\n",
    "    #\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    #\n",
    "    Final_words = []\n",
    "    #\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    #\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    #\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    #\n",
    "    for word, tag in pos_tag(entry):\n",
    "        #\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        #\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    #        \n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    #\n",
    "    Corpus.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1d35a",
   "metadata": {},
   "source": [
    "### Prepare Train and Test Data sets\n",
    "\n",
    "The Corpus will be split into two data sets, Training and Test. The training data set will be used to fit the model and the predictions will be performed on the test data set.This can be done through the train_test_split from the sklearn library. The Training Data will have 70% of the corpus and Test data will have the remaining 30% as we have set the parameter test_size=0.3 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a78331bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed542f4",
   "metadata": {},
   "source": [
    "Label encode the target variable — This is done to transform Categorical data of string type in the data set into numerical values which the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "867bb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y  = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc533c12",
   "metadata": {},
   "source": [
    "### Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "78c4bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf  = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "321e22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6358f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Train_X_Tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f968eec",
   "metadata": {},
   "source": [
    "### Use the ML Algorithms to Predict the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9b83123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  83.43333333333334\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bd71d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  84.2\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e889074",
   "metadata": {},
   "source": [
    "## Example 5 - Sentiment Analysis Deep Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b33f0",
   "metadata": {},
   "source": [
    "See notebooks 07-DLP-classifying-movie-revies both from the book of ***François Chollet \"Deep Learning with Python\"***:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1a02d",
   "metadata": {},
   "source": [
    "[click to open](./07-DLP-classifying-movie-reviews.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549755c",
   "metadata": {},
   "source": [
    "## References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c93f4c",
   "metadata": {},
   "source": [
    "***Bengfort B. et al.***, \"*Applied Text Analysis with Python*\" O'Reilly (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e7ef5",
   "metadata": {},
   "source": [
    "***Bird S. et al.***, \"*Natural Language Processing with Python*\" O'Reilly (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda46e1e",
   "metadata": {},
   "source": [
    "***Chollet F.***, \"*Deep Learning with Python*\" Manning (2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "7",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
