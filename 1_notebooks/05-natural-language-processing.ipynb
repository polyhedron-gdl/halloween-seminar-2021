{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01435e96",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/qf-workshop-2021/introduction-to-machine-learning/blob/main/1_notebooks/introduction-to-text-analysis.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ff4b8",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Tools and Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621aee4a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Natural language processing (NLP) is a field of machine learning in which computers analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.\n",
    "\n",
    "NLP is characterized as a difficult problem in computer science. Human language is rarely precise, or plainly spoken. To understand human language is to understand not only the words, but the concepts and how they’re linked together to create meaning. Despite language being one of the easiest things for the human mind to learn, the ambiguity of language is what makes natural language processing a difficult problem for computers to master.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60b82d",
   "metadata": {},
   "source": [
    "## The NLTK package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae37a5",
   "metadata": {},
   "source": [
    "What is NLTK? The [Natural Language Toolkit](https://www.nltk.org/), or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit, plus a text book available also on line [here](https://www.nltk.org/book/)\n",
    "\n",
    "NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems. There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities. \n",
    "\n",
    "The latest version is NLTK 3.3. It can be used by students, researchers, and industrialists. It is an Open Source and free library. It is available for Windows, Mac OS, and Linux. \n",
    "\n",
    "You can install nltk using pip installer if it is not installed in your Python installation. To test the installation:\n",
    "\n",
    "- Open your Python IDE or the CLI interface (whichever you use normally)\n",
    "- Type `import nltk` and press enter if no message of missing nltk is shown then nltk is installed on your computer.\n",
    "\n",
    "After installation, nltk also provides test datasets to work within Natural Language Processing. You can download it by using the following commands in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0744d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da2a82",
   "metadata": {},
   "source": [
    "### Using NLTK Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08f133",
   "metadata": {},
   "source": [
    "You can use the NLTK Text Corpora which is a vast repository for a large body of text called as a Corpus which can be used while you are working with Natural Language Processing (NLP) with Python. There are many different types of corpora available that you can use with varying types of projects, for example, a selection of free electronic books, web and chat text and news documents on different genres.\n",
    "\n",
    "In the online book site you can find everything you need to known to access the NLTK Corpus, in particular you can start from [Chapter 2 - Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html).\n",
    "\n",
    "Here is an example of how you can use a corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3972ee4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f977012",
   "metadata": {},
   "source": [
    "### Loading Your Own Corpus\n",
    "\n",
    "If you have your own collection of text files that you would like to access using the above methods, you can easily load them with the help of NLTK's `PlaintextCorpusReader`. Check the location of your files on your file system; in the following example, we have taken this to be the directory `C:\\Corpus\\EBA`. Whatever the location, set this to be the value of corpus_root.\n",
    "\n",
    "The second parameter of the PlaintextCorpusReader initializer can be a list of fileids, like ['a.txt', 'test/b.txt'], or a pattern that matches all fileids, like '[abc]/.*\\.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1e5125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Final Guidelines on Accounting for Expected Credit Losses (EBA-GL-2017-06).txt',\n",
       " 'Final Guidelines on the management of interest rate risk arising from non-trading activities.txt',\n",
       " 'Final Report on Guidelines on LGD estimates under downturn conditions.txt',\n",
       " 'Final Report on Guidelines on default definition (EBA-GL-2016-07).txt',\n",
       " 'Final Report on Guidelines on uniform disclosure of IFRS9 transitional arrangements (EBA-GL-2018-01).txt',\n",
       " 'Final report on updated GL Funding Plans (EBA 9.12.2019).txt',\n",
       " 'Final report on updated GL Funding Plans_(EBA-GL-2019-05)_09122019.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = '.\\corpus\\EBA'\n",
    "corpus_list = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n",
    "corpus_list.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c00b12",
   "metadata": {},
   "source": [
    "Let's pick out one of these text (for example the guideline on the interest rate), give it a short name, irrbb_w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3346b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"Final Guidelines on the management of interest rate risk arising from non-trading activities.txt\"\n",
    "irrbb_w = corpus_list.words(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dbedb6",
   "metadata": {},
   "source": [
    "For more information about corpora readers see also *Bengfort B. et al. \"Applied Text Analysis with Python\" O'Reilly (2018) Chapter 2*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d8829",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aceccef",
   "metadata": {},
   "source": [
    "In this section I want to go over some important NLP concepts and show code examples on how to apply them on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42647a50",
   "metadata": {},
   "source": [
    "Once you are sure that all documents loaded properly, go on to **preprocess your texts**.\n",
    "This step allows you to **remove numbers, dealing with capitalization, common words, punctuation, and otherwise prepare your texts for analysis**\n",
    "\n",
    "Data cleansing, though tedious, is perhaps the most important step in text analysis.   As we will see, dirty data can play havoc with the results.  Furthermore, as we will also see, data cleaning is invariably an iterative process as there are always problems that are overlooked the first time around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edc465",
   "metadata": {},
   "source": [
    "**Removing punctuation**:\n",
    "Your computer cannot actually read. Punctuation and other special characters only look like more words to your computer and Python. Use the following to methods to remove them from your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92a8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clear', 'and', 'effective', 'communication', 'is', 'very', 'important', 'to', 'Our', 'monetary', 'policy', 'becomes', 'more', 'effective', 'when', 'our', 'decisions', 'are', 'better', 'The', 'media', 'play', 'an', 'important', 'role', 'in', 'this', 'process', 'and', 'help', 'keep', 'us', 'accountable', 'to', 'the', 'European']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Clear and effective communication is very important to us. Our monetary policy becomes \\\n",
    "            more effective when our decisions are better understood. The media play an important \\\n",
    "            role in this process and help keep us accountable to the European public.\"\n",
    "tokens = sentence.split()\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3cdb0",
   "metadata": {},
   "source": [
    "Note that this result is not completely correct. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea5a6c",
   "metadata": {},
   "source": [
    "**Converting to lowercase**:\n",
    "As before, we want a word to appear exactly the same every time it appears. Since many languages **are** case sensitive, “Text” is not equal to “text” – and hence the rationale for converting to a standard case. We therefore change everything to lowercase. This is also a good time to check for any other special symbols that may need to be removed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "695d5d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clear', 'and', 'effective', 'communication', 'is', 'very', 'important', 'to', 'us', 'our', 'monetary', 'policy', 'becomes', 'more', 'effective', 'when', 'our', 'decisions', 'are', 'better', 'understood', 'the', 'media', 'play', 'an', 'important', 'role', 'in', 'this', 'process', 'and', 'help', 'keep', 'us', 'accountable', 'to', 'the', 'european', 'public']\n"
     ]
    }
   ],
   "source": [
    "# all lowercase\n",
    "words = [word.lower() for word in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38948b",
   "metadata": {},
   "source": [
    "**Removing numbers**: Text analysts are typically not interested in numbers since these do not usually contribute to the meaning of the text. **However, this may not always be so. For example, it is definitely not the case if one is interested in getting a count of the number of times a particular year appears in a corpus**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4be488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers\n",
    "words = [w for w in words if not w.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89860dc",
   "metadata": {},
   "source": [
    "**Removing “Stop Words” (common words)** \n",
    "\n",
    "In every text, there are a lot of common, and uninteresting words that we generically call Stop WOrds. Stop words are words that may not carry any valuable information, like articles (“the”), conjunctions (“and”), or propositions (“with”). Why would you want to remove them? Because finding out that “the” and “a” are the most common words in your dataset doesn’t tell you much about the data. Such words are frequent by their nature, and will confound your analysis if they remain in the text. \n",
    "\n",
    "NLP Python libraries like NLTK usually come with an in-built stopword list which you can easily import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab375bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4367f9e",
   "metadata": {},
   "source": [
    "Note, that in some cases stop words matter. For example, in identifying negative reviews or recommendations. People will use stop words like “no” and “not” in negative reviews: “I will not buy this product again. I saw no benefits in using it”. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340f498",
   "metadata": {},
   "source": [
    "**Removing particular words**:\n",
    "\n",
    "The NLTK stopword list, however, only has around 200 stopwords. The stopword list which one can commonly use for text analysis may contains almost 600 words. So if you find that a particular word or two appear in the output, but are not of value to your particular analysis, you can remove them, specifically, from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfa96f",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is a process of splitting a text object into smaller units which are also called tokens. Examples of tokens can be words, numbers, engrams, or even symbols. Single words are called unigrams, two words bi-grams, and three words tri-grams.\n",
    "\n",
    "The most commonly used tokenization process is White-space Tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc5eadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clear', 'and', 'effective', 'communication', 'is', 'very', 'important', 'to', 'us.', 'Our', 'monetary', 'policy', 'becomes', 'more', 'effective', 'when', 'our', 'decisions', 'are', 'better', 'understood.', 'The', 'media', 'play', 'an', 'important', 'role', 'in', 'this', 'process', 'and', 'help', 'keep', 'us', 'accountable', 'to', 'the', 'European', 'public.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Clear and effective communication is very important to us. Our monetary policy becomes more \\\n",
    "            effective when our decisions are better understood. The media play an important role in this \\\n",
    "            process and help keep us accountable to the European public.\"\n",
    "tokens = sentence.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f960ec5",
   "metadata": {},
   "source": [
    "When dealing with a new dataset it's often helpful to extract the most common words to get an idea of what the data is about. You usually want to extract the most common unigrams first, but it can also be useful to extract n-grams with larger n to identify patterns. NLTK has in-built bigrams, trigrams and ngrams functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9066ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "sen = \"Clear and effective communication is very important to us. Our monetary policy becomes more \\\n",
    "            effective when our decisions are better understood. The media play an important role in this \\\n",
    "            process and help keep us accountable to the European public.\"\n",
    "nltk_tokens = word_tokenize(sen) #using tokenize from NLKT and not split() because split() does not take into account punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36256e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clear', 'and', 'effective', 'communication', 'is', 'very', 'important', 'to', 'us', 'Our', 'monetary', 'policy', 'becomes', 'more', 'effective', 'when', 'our', 'decisions', 'are', 'better', 'understood', 'The', 'media', 'play', 'an', 'important', 'role', 'in', 'this', 'process', 'and', 'help', 'keep', 'us', 'accountable', 'to', 'the', 'European', 'public']\n"
     ]
    }
   ],
   "source": [
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in nltk_tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698aad2",
   "metadata": {},
   "source": [
    "This is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a79a2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Clear', 'and'), ('and', 'effective'), ('effective', 'communication'), ('communication', 'is'), ('is', 'very'), ('very', 'important'), ('important', 'to'), ('to', 'us'), ('us', '.'), ('.', 'Our'), ('Our', 'monetary'), ('monetary', 'policy'), ('policy', 'becomes'), ('becomes', 'more'), ('more', 'effective'), ('effective', 'when'), ('when', 'our'), ('our', 'decisions'), ('decisions', 'are'), ('are', 'better'), ('better', 'understood'), ('understood', '.'), ('.', 'The'), ('The', 'media'), ('media', 'play'), ('play', 'an'), ('an', 'important'), ('important', 'role'), ('role', 'in'), ('in', 'this'), ('this', 'process'), ('process', 'and'), ('and', 'help'), ('help', 'keep'), ('keep', 'us'), ('us', 'accountable'), ('accountable', 'to'), ('to', 'the'), ('the', 'European'), ('European', 'public'), ('public', '.')]\n",
      "[('Clear', 'and', 'effective'), ('and', 'effective', 'communication'), ('effective', 'communication', 'is'), ('communication', 'is', 'very'), ('is', 'very', 'important'), ('very', 'important', 'to'), ('important', 'to', 'us'), ('to', 'us', '.'), ('us', '.', 'Our'), ('.', 'Our', 'monetary'), ('Our', 'monetary', 'policy'), ('monetary', 'policy', 'becomes'), ('policy', 'becomes', 'more'), ('becomes', 'more', 'effective'), ('more', 'effective', 'when'), ('effective', 'when', 'our'), ('when', 'our', 'decisions'), ('our', 'decisions', 'are'), ('decisions', 'are', 'better'), ('are', 'better', 'understood'), ('better', 'understood', '.'), ('understood', '.', 'The'), ('.', 'The', 'media'), ('The', 'media', 'play'), ('media', 'play', 'an'), ('play', 'an', 'important'), ('an', 'important', 'role'), ('important', 'role', 'in'), ('role', 'in', 'this'), ('in', 'this', 'process'), ('this', 'process', 'and'), ('process', 'and', 'help'), ('and', 'help', 'keep'), ('help', 'keep', 'us'), ('keep', 'us', 'accountable'), ('us', 'accountable', 'to'), ('accountable', 'to', 'the'), ('to', 'the', 'European'), ('the', 'European', 'public'), ('European', 'public', '.')]\n"
     ]
    }
   ],
   "source": [
    "#splitting sentence into bigrams and trigrams\n",
    "print(list(bigrams(nltk_tokens)))\n",
    "print(list(trigrams(nltk_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3c266",
   "metadata": {},
   "source": [
    "The sents() function divides the text up into sentencese, where each sentence is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ca460f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GUIDELINES', 'ON', 'THE', 'MANAGEMENT', 'OF', 'INTEREST', 'RATE', 'RISK'], ['ARISING', 'FROM', 'NON', '-', 'TRADING', 'BOOK', 'ACTIVITIES'], ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrbb_s = corpus_list.sents(id)\n",
    "irrbb_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ea1e0",
   "metadata": {},
   "source": [
    "## Lemmatisation and Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d739b",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's.\n",
    "\n",
    "Languages we speak and write are made up of several words often derived from one another. When a language contains words that are derived from another word as their use in the speech changes is called **Inflected Language**. [Inflection](https://en.wikipedia.org/wiki/Inflection) is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change.\n",
    "\n",
    "Typically a large corpus will contain many words that have a common root – for example: offer, offered and offering. Lemmatisation and stemming both refer to a process of reducing a word to its root. The difference is that stem might not be an actual word whereas, a lemma is an actual word. It’s a handy tool if you want to avoid treating different forms of the same word as different words. Let's consider the following example:\n",
    "\n",
    "- Lemmatising: considered, considering, consider → “consider”\n",
    "- Stemming: considered, considering, consider → “consid”\n",
    "\n",
    "Stemming and Lemmatization are widely used in tagging systems, indexing, SEOs, Web search results, and information retrieval. For example, searching for fish on Google will also result in fishes, fishing as fish is the stem of both words.\n",
    "\n",
    "NLTK comes with many different in-built lemmatisers and stemmers, so just plug and play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b175e",
   "metadata": {},
   "source": [
    "### NLTK Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af6303e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consid\n",
      "considering\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = \"considering\"\n",
    "\n",
    "stemmed_word =  stemmer.stem(word)\n",
    "lemmatised_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "print(stemmed_word)\n",
    "print(lemmatised_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489022c8",
   "metadata": {},
   "source": [
    "We can use any of the text of nltk corpora to make some test. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e345d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file=nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "my_lines_list=[]\n",
    "for line in text_file:\n",
    "    my_lines_list.append(line)\n",
    "#my_lines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7fd31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18b5eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_file=open(\"stemming_example.txt\",mode=\"a+\", encoding=\"utf-8\")\n",
    "\n",
    "for line in my_lines_list:\n",
    "    stem_sentence=stemSentence(line)\n",
    "    stem_file.write(stem_sentence)\n",
    "    \n",
    "stem_file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877fc11",
   "metadata": {},
   "source": [
    "Python nltk provides not only two English stemmers: PorterStemmer and LancasterStemmer but also a lot of non-English stemmers as part of SnowballStemmers, ISRIStemmer, RSLPSStemmer. Python NLTK included SnowballStemmers as a language to create to create non-English stemmers. One can program one's own language stemmer using snowball. Currently, it supports the following languages:\n",
    "- Danish\n",
    "- Dutch\n",
    "- English\n",
    "- French\n",
    "- German\n",
    "- Hungarian\n",
    "- Italian\n",
    "- Norwegian\n",
    "- Porter\n",
    "- Portuguese\n",
    "- Romanian\n",
    "- Russian\n",
    "- Spanish\n",
    "- Swedish\n",
    "\n",
    "ISRIStemmer is an Arabic stemmer and RSLPStemmer is stemmer for the Portuguese Language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495aff3",
   "metadata": {},
   "source": [
    "### NLTK Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d098816",
   "metadata": {},
   "source": [
    "As we have already said, lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.\n",
    "\n",
    "Python NLTK provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words. WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29df92b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a8d55",
   "metadata": {},
   "source": [
    "In the above output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in `wordnet_lemmatizer.lemmatize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45bff564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c816b92",
   "metadata": {},
   "source": [
    "### Stemming or lemmatization?\n",
    "\n",
    "After going through this section, you may be asking yourself when should I use Stemming and when should I use Lemmatization? We have seen the following points:\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "So when to use what! The above points show that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumed time and processing. It depends on the application you are working on that decides if stemmers should be used or lemmatizers. If you are building a language application in which language is important you should use lemmatization as it uses a corpus to match root forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983daca",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b1f67",
   "metadata": {},
   "source": [
    "### What is POS Tagging?\n",
    "\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging or POS-tagging, or simply tagging. The part of speech **explains how a word is used in a sentence**. There are eight main parts of speech: \n",
    "\n",
    "- nouns, \n",
    "- pronouns, \n",
    "- adjectives, \n",
    "- verbs, \n",
    "- adverbs, \n",
    "- prepositions, \n",
    "- conjunctions\n",
    "- interjections.\n",
    "\n",
    "Examples:\n",
    "\n",
    " - **Noun (N)** : Daniel, London, table, dog, teacher, pen, city\n",
    " - **Verb (V)** : go, speak, run, eat, play, live, walk, have, like, are, is\n",
    " - **Adjective (ADJ)** : big, happy, green, young, fun, crazy, three\n",
    " - **Adverb (ADV)** : slowly, quietly, very, always, never, too, well, tomorrow\n",
    " - **Preposition (P)** : at, on, in, from, with, near, between, about, under\n",
    " - **Conjunction (CON)** : and, or, but, because, so, yet, unless, since, if\n",
    " - **Pronoun (PRO)** : I, you, we, they, he, she, it, me, us, them, him, her, this\n",
    " - **Interjection (INT)** : Ouch! Wow! Great! Help! Oh! Hey! Hi!\n",
    "\n",
    "Most POS are divided into sub-classes. POS Tagging simply means labeling words with their appropriate Part-Of-Speech.\n",
    "\n",
    "The collection of tags used for a particular task is known as a **Tagset**.\n",
    "\n",
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word.\n",
    "Lets first run the below code and see what exactly are we talking about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4040b",
   "metadata": {},
   "source": [
    "POS tagging is a **supervised learning solution** that uses features like the previous word, next word, is first letter capitalized etc. NLTK has a function to get pos tags and it works after tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6590468a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Giovanni']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"My name is Giovanni\"\n",
    "token = nltk.word_tokenize(sentence)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77ed8e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Giovanni', 'NNP')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5496c72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n"
     ]
    }
   ],
   "source": [
    "# We can get more details about any POS tag using help funciton of NLTK as follows.\n",
    "nltk.help.upenn_tagset(\"PRP$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35446d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NN$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae9a5dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"VBZ$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e143223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NNP$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5555019",
   "metadata": {},
   "source": [
    "The most popular tag set is Penn Treebank tagset. Most of the already trained taggers for English are trained on this tag set. To view the complete list, follow this [link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). Uncomment the below code to have the complete upenn tagset used in the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de25b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('tagsets')\n",
    "#print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b18fb0",
   "metadata": {},
   "source": [
    "### Example 1 - Reading the Newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e88153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrats took exceedingly narrow control of the Senate on Wednesday after winning both runoff elections in Georgia, granting them control of Congress and the White House for the first time since 2011. Democrat Jon Ossoff defeated Republican David Perdue, according to The Associated Press, making him the youngest member of the U.S. Senate and the first Jewish senator from Georgia. Earlier Raphael Warnock, a pastor from Atlanta, defeated GOP Sen. Kelly Loeffler after a bitter campaign. Warnock becomes the first Black Democrat elected to the Senate from a Southern state.The Senate will now be split 50-50 between the two parties, giving Vice President-elect Kamala Harris the tiebreaking vote.\n"
     ]
    }
   ],
   "source": [
    "article = \"Democrats took exceedingly narrow control of the Senate on Wednesday after winning both runoff elections in Georgia, granting them control of Congress and the White House for the first time since 2011. Democrat Jon Ossoff defeated Republican David Perdue, according to The Associated Press, making him the youngest member of the U.S. Senate and the first Jewish senator from Georgia. Earlier Raphael Warnock, a pastor from Atlanta, defeated GOP Sen. Kelly Loeffler after a bitter campaign. Warnock becomes the first Black Democrat elected to the Senate from a Southern state.The Senate will now be split 50-50 between the two parties, giving Vice President-elect Kamala Harris the tiebreaking vote.\"\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb39979",
   "metadata": {},
   "source": [
    "At a first glance as \"human beings\", we can see that in the extract there are different names of people and also of organizations; although it is possible that the names are completely new to us, we can recognize without a shadow of a doubt that \"Kamala Harris\" is the name of a person of female gender, just as \"Associated Press\" could be the name of an organization or a conference. Now, we want the same result to be produced by our system. Like? Through a function that takes the sentence as input and is able to tokenize the content and extract the grammatical types. \n",
    "\n",
    "We can use the function ***pos_tag*** of the ***NLTK package***; it is in fact able to take an array of words and assign them a label that determines the type of content that each of them represents at the grammatical level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "501f9de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Democrats', 'NNPS'), ('took', 'VBD'), ('exceedingly', 'RB'), ('narrow', 'JJ'), ('control', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Senate', 'NNP'), ('on', 'IN'), ('Wednesday', 'NNP'), ('after', 'IN'), ('winning', 'VBG'), ('both', 'DT'), ('runoff', 'NN'), ('elections', 'NNS'), ('in', 'IN'), ('Georgia', 'NNP'), (',', ','), ('granting', 'VBG'), ('them', 'PRP'), ('control', 'NN'), ('of', 'IN'), ('Congress', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('White', 'NNP'), ('House', 'NNP'), ('for', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('time', 'NN'), ('since', 'IN'), ('2011', 'CD'), ('.', '.'), ('Democrat', 'NNP'), ('Jon', 'NNP'), ('Ossoff', 'NNP'), ('defeated', 'VBD'), ('Republican', 'NNP'), ('David', 'NNP'), ('Perdue', 'NNP'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('The', 'DT'), ('Associated', 'NNP'), ('Press', 'NNP'), (',', ','), ('making', 'VBG'), ('him', 'PRP'), ('the', 'DT'), ('youngest', 'JJS'), ('member', 'NN'), ('of', 'IN'), ('the', 'DT'), ('U.S.', 'NNP'), ('Senate', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('first', 'JJ'), ('Jewish', 'JJ'), ('senator', 'NN'), ('from', 'IN'), ('Georgia', 'NNP'), ('.', '.'), ('Earlier', 'RBR'), ('Raphael', 'NNP'), ('Warnock', 'NNP'), (',', ','), ('a', 'DT'), ('pastor', 'NN'), ('from', 'IN'), ('Atlanta', 'NNP'), (',', ','), ('defeated', 'VBD'), ('GOP', 'NNP'), ('Sen.', 'NNP'), ('Kelly', 'NNP'), ('Loeffler', 'NNP'), ('after', 'IN'), ('a', 'DT'), ('bitter', 'JJ'), ('campaign', 'NN'), ('.', '.'), ('Warnock', 'NNP'), ('becomes', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('Black', 'NNP'), ('Democrat', 'NNP'), ('elected', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('Senate', 'NNP'), ('from', 'IN'), ('a', 'DT'), ('Southern', 'NNP'), ('state.The', 'NN'), ('Senate', 'NNP'), ('will', 'MD'), ('now', 'RB'), ('be', 'VB'), ('split', 'VBN'), ('50-50', 'JJ'), ('between', 'IN'), ('the', 'DT'), ('two', 'CD'), ('parties', 'NNS'), (',', ','), ('giving', 'VBG'), ('Vice', 'NNP'), ('President-elect', 'NNP'), ('Kamala', 'NNP'), ('Harris', 'NNP'), ('the', 'DT'), ('tiebreaking', 'JJ'), ('vote', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.word_tokenize(article)\n",
    "tagged_tokens = nltk.pos_tag(tagged_tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52e729",
   "metadata": {},
   "source": [
    "As we have already seen, at a first glance, everything appears very confusing, but in reality each of those acronyms has a very specific meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f7dde",
   "metadata": {},
   "source": [
    "<p><strong>NLTK POS Tags Examples are as Below:</strong></p>\n",
    "<table class=\"table table-striped\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th  style=\"text-align: left;\">Abbreviation</th>\n",
    "<th  style=\"text-align: left;\">Meaning</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">CC\n",
    "</td>\n",
    "<td style=\"text-align: left;\">coordinating conjunction\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">CD\n",
    "</td>\n",
    "<td style=\"text-align: left;\">cardinal digit\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">DT\n",
    "</td>\n",
    "<td style=\"text-align: left;\">determiner\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">EX\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">existential there\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">FW\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">foreign word\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">IN\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">preposition/subordinating conjunction\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">JJ\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">This NLTK POS Tag is an adjective (large)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">JJR\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adjective, comparative (larger)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">JJS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adjective, superlative (largest)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">LS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">list market\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">MD\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">modal (could, will)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">NN\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">noun, singular (cat, tree)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">NNS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">noun plural (desks)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">NNP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">proper noun, singular (sarah)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">NNPS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">proper noun, plural (indians or americans)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">PDT\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">predeterminer (all, both, half)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">POS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">possessive ending  (parent\\ &#8216;s)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">PRP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">personal pronoun (hers, herself, him,himself)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">PRP$\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">possessive pronoun (her, his, mine, my, our )\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">RB\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adverb (occasionally, swiftly)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">RBR\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adverb, comparative (greater)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">RBS\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">adverb, superlative (biggest)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">RP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">particle (about)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">TO\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">infinite marker (to)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">UH\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">interjection (goodbye)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VB\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb (ask)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBG\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb gerund (judging)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBD\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb past tense (pleaded)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBN\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb past participle (reunified)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb, present tense not 3rd person singular(wrap)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">VBZ\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">verb, present tense with 3rd person singular (bases)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">WDT\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">wh-determiner (that, what)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">WP\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">wh- pronoun (who)\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  style=\"text-align: left;\">WRB\n",
    "</td>\n",
    "<td  style=\"text-align: left;\">wh- adverb (how)\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99a19b",
   "metadata": {},
   "source": [
    "### Where we can use POST\n",
    "\n",
    "Part-of-speech tags describe the characteristic structure of lexical terms within a sentence or text, therefore, we can use them for making assumptions about semantics. Other applications of POS tagging include:\n",
    "\n",
    "- **Named Entity Recognition and Chunking (see notebook 4-1)**\n",
    "- Co-reference Resolution\n",
    "- Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8ebfa",
   "metadata": {},
   "source": [
    "### Build your own POS Tagger "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf596d5",
   "metadata": {},
   "source": [
    "It is interesting to understand how to create your own POS tagger using a supervised learning method. This will also allow us to understand the role played by features in identifying individual parts of the text. Finally we will see how the context plays a fundamental role for a correct classification. For this exercise we focus out attenction on suffixes. We'll train a classifier to work out which suffixes are most informative. The training sample will be build from the Brown Corpus which is part of the NLTK package.\n",
    "\n",
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on.  (for a complete list, see http://icame.uib.no/brown/bcm-los.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01504d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b81add",
   "metadata": {},
   "source": [
    "Let's begin by finding out what the most common suffixes are. For this we are going to use a FreqDist() object from the NLTK package.\n",
    "\n",
    "A frequency distribution counter records the number of times each outcome of an experiment has occurred. For example, a frequency distribution could be used to record the frequency of each word type in a document. Formally, a frequency distribution can be defined as a function mapping from each sample to the number of times that sample occurred as an outcome.\n",
    "\n",
    "Frequency distributions are generally constructed by running a number of experiments, and incrementing the count for a sample every time it is an outcome of an experiment. For example, the following code will produce a frequency distribution of any n-gram with $1 \\le n \\le 3$ appearing at the end of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55d04bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_fdist = nltk.FreqDist()\n",
    "\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1   # single character at the end of the word\n",
    "    suffix_fdist[word[-2:]] += 1   # bi-gram\n",
    "    suffix_fdist[word[-3:]] += 1   # three-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ce4169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'e': 202946, ',': 175002, '.': 152999, 's': 128722, 'd': 105687, 't': 94459, 'he': 92084, 'n': 87889, 'a': 74912, 'of': 72978, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16962cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d67e7",
   "metadata": {},
   "source": [
    "Next, we'll define a feature extractor function which checks a given word for these suffixes. For this we are going to use the `endswith()` method of `String` class, that returns `True` if the string ends with the specified value, otherwise `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef61c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"processing\"\n",
    "word.endswith(\"ing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feed9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0007ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb9b12aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca21681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14fc3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31097a5f",
   "metadata": {},
   "source": [
    "Now that we've defined our feature extractor, we can use it to train a new **decision tree** classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9437a3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6270512182993535"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc4a2e",
   "metadata": {},
   "source": [
    "#### Exploiting Context\n",
    "\n",
    "By augmenting the feature extraction function, we could modify this part-of-speech tagger to leverage a variety of other word-internal features, such as the length of the word, the number of syllables it contains, or its prefix. However, as long as the feature extractor just looks at the target word, we have no way to add features that depend on the context that the word appears in. But contextual features often provide powerful clues about the correct tag — for example, when tagging the word \"fly,\" knowing that the previous word is \"a\" will allow us to determine that it is functioning as a noun, not a verb.\n",
    "\n",
    "In order to accommodate features that depend on a word's context, we must revise the pattern that we used to define our feature extractor. Instead of just passing in the word to be tagged, we will pass in a complete (untagged) sentence, along with the index of the target word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bfe97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97d4d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append( (pos_features_2(untagged_sent, i), tag) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b05b6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97cfdae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb04a5e",
   "metadata": {},
   "source": [
    "It is clear that exploiting contextual features improves the performance of our part-of-speech tagger. For example, the classifier learns that a word is likely to be a noun if it comes immediately after the word \"large\". However, it is unable to learn the generalization that a word is probably a noun if it follows an adjective, because it doesn't have access to the previous word's part-of-speech tag. In general, simple classifiers always treat each input as independent from all other inputs. In many contexts, this makes perfect sense. For example, decisions about whether names tend to be male or female can be made on a case-by-case basis. However, there are often cases, such as part-of-speech tagging, where we are interested in solving classification problems that are closely related to one another. [rinviare al paragrafo di nltk per i sequence classificators]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadbbe6",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac3e46",
   "metadata": {},
   "source": [
    "### What is spaCy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea0240",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io/) is another free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems. It is a fully optimized and  is widely used in deep learning.\n",
    "\n",
    "Both libraries (spaCy and NLTK) are excellent and very well managed and documented. Any of them could be a perfect choice, depending on the characteristics of your natural language processing projects. NLTK is a string processing library: it takes strings as input and returns the resulting string as output; spaCy uses an object-oriented approach. As a result spaCy returns an object in the form of a document, with words or phrases. NLTK provides multiple algorithms for solving a problem, while spaCy only provides the best algorithm for solving a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139233cf",
   "metadata": {},
   "source": [
    "We will not go into the details of this library in this introductory course. However we want to point out that the efficiency of the underlying deep learning models often make it the best choice for industrial applications. \n",
    "\n",
    "We will use some of the trained pipelines in the last part of this seminar where we talk about extracting information from unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4ff95",
   "metadata": {},
   "source": [
    "## References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e7ef5",
   "metadata": {},
   "source": [
    "***Bird S. et al.***, \"*Natural Language Processing with Python*\" O'Reilly (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c93f4c",
   "metadata": {},
   "source": [
    "***Bengfort B. et al.***, \"*Applied Text Analysis with Python*\" O'Reilly (2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "5",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
